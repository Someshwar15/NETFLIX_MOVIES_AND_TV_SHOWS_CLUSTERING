{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Someshwar15/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING/blob/main/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING_Capstone4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **NETFLIX MOVIES AND TV SHOWS CLUSTERING**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Team\n",
        "##### **Team Member 1 -**Someshwar S Hydrabade\n",
        "##### **Team Member 2 -**Sarvesha D Kapse\n"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![download.webp](data:image/webp;base64,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)\n",
        "\n",
        "Netflix, the world’s largest on-demand internet streaming media and online DVD movie rental service provider.it Founded August 29, 1997, in Los Gatos, California by Marc and Reed. It has 69 million members in over 60 countries enjoying more than 100 million hours of TV shows and movies per day Netflix is the world’s leading internet entertainment service with enjoying TV series, documentaries, and feature films across a wide variety of genres and languages. I was curious to analyze the content released in Netflix platform which led me to create these simple, interactive, and exciting visualizations and find similar groups of people.\n",
        "\n",
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Fixable which is a third-party Netflix search engine. In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset. Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Someshwar's link :- https://github.com/Someshwar15/NETFLIX_MOVIES_AND_TV_SHOWS_CLUSTERING\n",
        "####Sarvesha's link  :-"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset consists of tv shows and movies available on Netflix as of 2019. The dataset is collected from Flixable which is a third-party Netflix search engine.\n",
        "\n",
        "In 2018, they released an interesting report which shows that the number of TV shows on Netflix has nearly tripled since 2010. The streaming service’s number of movies has decreased by more than 2,000 titles since 2010, while its number of TV shows has nearly tripled. It will be interesting to explore what all other insights can be obtained from the same dataset.\n",
        "\n",
        "Integrating this dataset with other external datasets such as IMDB ratings, rotten tomatoes can also provide many interesting findings.."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "# importing all necessory libraries and\n",
        "# install packages\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import matplotlib.cm as cm\n",
        "import missingno as msno\n",
        "import plotly.express as px\n",
        "%matplotlib inline\n",
        "\n",
        "import re\n",
        "import plotly.graph_objects as go\n",
        "from plotly.offline import init_notebook_mode, iplot\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from scipy import stats\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.metrics import silhouette_samples\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
        "from PIL import Image\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing dataset\n",
        "\n",
        "df = pd.read_csv(r'/content/drive/MyDrive/netflix/NETFLIX MOVIES AND TV SHOWS CLUSTERING.csv')"
      ],
      "metadata": {
        "id": "uyHKfzUwVXXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "check_dupl = df.duplicated(keep = False).any()\n",
        "if check_dupl == False:\n",
        "    print('There are no duplicate rows in our dataset')\n",
        "else:\n",
        "    print ('There are duplicate rows in our dataset')"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "null = pd.DataFrame({'No of Total values': df.shape[0] , 'No of NaN Values': df.isnull().sum(),\n",
        "                     '% of NaN values': round((df.isnull().sum()/df.shape[0])*100, 2)})\n",
        "null.sort_values('No of NaN Values', ascending = False)"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "director column has highest *30.68%* NaN values.\n",
        "\n",
        "cast column has *9.22%* NaN values.\n",
        "\n",
        "country, date_added, rating columns also containing NaN values."
      ],
      "metadata": {
        "id": "cu9DlPlRXFv3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "Hg09dY4rXhPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ploting null values present in dataset\n",
        "\n",
        "nan = df.isna()\n",
        "nan.head()"
      ],
      "metadata": {
        "id": "MleC_VzWXlKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.heatmap(nan)\n",
        ""
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The given dataset is consist the TV shows and Movies available on Netflix as of 2019. This dataset is about the content that is present on the Netflix platform and the details of it like title, cast, director, release year, rating etc.\n",
        "\n",
        "The dataset consist 12 columns and 7787 rows in it and data is in textual from that is object data type and only release year column is numerical type.\n",
        "\n",
        "Dataset does not content any duplicate values but it has some missing values/ null values in it. So we need to treat the all neccesory process and NLP before applying the clutering algorithms."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "\n",
        "\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we see in the dataframe that only 'release_year' column contains the numeric data so it showing only that columns describtion."
      ],
      "metadata": {
        "id": "FejL7HkmYMGO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe(include= 'O').T"
      ],
      "metadata": {
        "id": "rGjVuxj3YUmS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**show_id** : Unique ID for every Movie / Tv Show\n",
        "\n",
        "**type** : Identifier - A Movie or TV Show\n",
        "\n",
        "**title** : Title of the Movie / Tv Show\n",
        "\n",
        "**director** : Director of the Movie / TV Show\n",
        "\n",
        "**cast** : Actors involved in the movie / show\n",
        "\n",
        "**country** : Country where the movie / show was produced\n",
        "\n",
        "**date_added** : Date it was added on Netflix\n",
        "\n",
        "**release_year** : Actual Release year of the movie / show\n",
        "\n",
        "**rating** : TV Rating of the movie / show\n",
        "\n",
        "**duration** : Total Duration - in minutes or number of seasons\n",
        "\n",
        "**listed_in** : Genre\n",
        "\n",
        "**description**: The Summary description"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df_copy = df.copy()\n",
        "df_copy['director'].fillna('unknown', inplace =True)\n",
        "df_copy['cast'].fillna('unknown', inplace =True)\n",
        "df_copy['country'].fillna('unknown', inplace =True)\n",
        "df.dropna(subset=['date_added', 'rating'], axis=0, inplace=True)\n",
        "df_copy['date_added'] = pd.to_datetime(df_copy['date_added'])\n",
        "df_copy ['listed_in'] = df_copy.listed_in.apply(lambda row: row.split(', '))\n",
        "df_copy.explode('listed_in')['listed_in'].unique()"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have apply the fillna and dropna method for the attributes that content the null or missing values.\n",
        "\n",
        "We convert date added attribute as datetime Dtype"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. ***Discovering Information -  EDA***\n",
        "Now, we hope our data is ready to analyse. Our job is to find useful insights from the data. For this we will begin with univariate analysis for each variables then we will move to bivariate and multivariate analysis.\n",
        "\n",
        "### Univariate Analysis\n",
        "\n",
        "#### 1. Distribution of Type of Content : TV Shows and Movies"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 1 -  Types of Netflix Content"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "plt.figure(figsize=(14, 7))\n",
        "labels=['TV Show', 'Movie']\n",
        "plt.pie(df['type'].value_counts().sort_values(),labels=labels,explode=[0.01,0.01],\n",
        "        autopct='%1.2f%%',colors=sns.color_palette(\"flare\",2), startangle=90)\n",
        "plt.title('Types of Netflix Content')\n",
        "plt.axis('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To better visualize the content that present in the TV shows and Movies we plot the pie chart."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There is around 69% videos present that are Movies and remaining 31% is as TV Show.\n",
        "From the pie chart we can see that the movies having the highest no of content than the tv shows."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help to understand the process well in less time of period and it give more insight of the observation to the company"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 2 - Types of video present on Netflix"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(x=df_copy['type'], data=df_copy, palette='icefire')\n",
        "plt.xlabel('Type')\n",
        "plt.ylabel('Number of Shows and Movies')\n",
        "plt.title('Types of Entertainment Video on Netflix')\n",
        "plt.show();\n",
        "content = df_copy['type'].value_counts()\n",
        "content"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We need the check which type has higher number of videos or the content on the Netflix."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart we know that the highest number of videos present on the Netflix is from Movies type, it has almost the double of the videos that present in the TV show type."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help the company to understand the which type has more content in it on there platform and it will help to improve in certain aspect to reach there desirable goals."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  3 - Release Year of Content on Netflix"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "# histogram for release dates of movies and tv shows\n",
        "\n",
        "ticks=np.arange(df_copy.release_year.min(), df_copy.release_year.max()+1,1)\n",
        "plt.figure(figsize=(15,9))\n",
        "sns.displot(x='release_year', data= df_copy, kind='hist', height=10, aspect=2, bins=50, hue='type', multiple='dodge', palette='GnBu')\n",
        "plt.xticks(ticks, rotation=90)\n",
        "plt.xlabel('Release Year')\n",
        "plt.title('Distribution of Movies & TV Shows Release Year')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the chart to find out the distribution of Movies & TV Shows Release Year."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the movies were released on the platform after 2012\n",
        "\n",
        "Most of the TV Shows were released on the platform after 2015\n",
        "\n",
        "Highest number of Movies and TV Shows were released on the platform on 2017 year\n",
        "\n",
        "From above observations we can see that the Netflix has been increasingly focusing on TV Shows rathar than the Movies in recent year."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help the company to understand the number of content that available on the platform is released on which year and the which type is on more focus in recent years from Netflix."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart 4 - Rating of Content on Netflix"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "plt.figure(figsize=(15,9))\n",
        "a = df_copy.groupby('rating').agg({'show_id':'count'}).reset_index()\n",
        "sns.barplot(x=a['rating'], y=a['show_id'], palette='bone')\n",
        "plt.xlabel('Ratings')\n",
        "plt.ylabel('Num of Videos')\n",
        "plt.title('Rating Distribution of TV Shows and Movies on Netflix')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the rating distribution of TV Shows and Movies on Netflix we plot this barplot."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most of the content on Netflix is rated for *above 14 year old* audiance.\n",
        "From the barplot we see that the most of the content present on the Netflix is for adult and the above 14 year old audiance"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help the company to understand the which rating has more numbers of content availables on there platform and can imorove on there aspect of producing the ratingwise content on Netflix."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  5 - Distribution Season of TV Shows"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "tv_show = df_copy[df_copy['type']== 'TV Show']\n",
        "plt.figure(figsize= (30,6))\n",
        "sns.countplot(x = tv_show['duration'], data= tv_show, order = tv_show['duration'].value_counts().index)\n",
        "plt.title('Distribution of TV Show Durations')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize from this chart distribution of TV shows Duration(Seasons)."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest number of TV Shows are having only one season.\n",
        "From the chart TV shows present on Netflix vaires from 1 season to 16 seasons.\n",
        "There is very few TV shows that have more than Eight seasons."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above chart will help the company to visualize that the how much avg seasons are uploaded in TV Show type. It will help to gain insight that the how many seasons are released by the TV series."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  6 - Production Growth Yearly"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# preaparing data to visualize the chart\n",
        "\n",
        "year_movie = df_copy[df_copy.type =='Movie']['release_year'].value_counts().sort_index(ascending=False).head(10)\n",
        "year_show = df_copy[df_copy.type =='TV Show']['release_year'].value_counts().sort_index(ascending=False).head(10)\n",
        "total = df_copy['release_year'].value_counts().sort_index(ascending=False).head(10)\n",
        ""
      ],
      "metadata": {
        "id": "hR-Tt2XZf7nn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "sns.set(font_scale = 1)\n",
        "total.plot(figsize=(10,5), linewidth= 1.5, color='red', label='Total content per year')\n",
        "year_movie.plot(figsize=(10,5), linewidth= 1.5, color='green', label='Movies per year')\n",
        "year_show.plot(figsize=(10,5), linewidth= 1.5, color='yellow', label='TV Show per year')\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Number')\n",
        "plt.legend()\n",
        "plt.title('Production yearly', fontsize=20)\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the chart to check the production growth based on type of the content and release year."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Production of the total no of content on Netflix is higher on the year of 2018.\n",
        "\n",
        "Production of the Movies on Netflix is higher from *2016* to 2018 year.\n",
        "\n",
        "Production of TV Shows on Netflix is higher from 2018 to 2020 year."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart is help to understand the company that the Movies and TV Shows production how much grows yearly and which type production is significantly grows in years. And also the total production of the years."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  7 - Top Content Producing Countries"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Countires by the content produce on netflix\n",
        "\n",
        "countries = df_copy.set_index('title').country.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "countries = countries[countries != 'unknown']\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y=countries, order=countries.value_counts().index[:15])\n",
        "plt.xlabel('Title')\n",
        "plt.ylabel('Country')\n",
        "plt.title('Top 15 Countires')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the countires by the content produce on Netflix we plot the bar and pie chart."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the chart we can see the top 15 countries contributed on netflix and United State is highest number of content created country on the netflix followed by India."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The charts is help the company to better understanding the which country has more content produce on Netflix platform and improve there marketing strategy in repective countries."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  8 -  Content Producing Countries Globaly"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# distribution of content on basis of countries\n",
        "\n",
        "country_all = df.groupby(['country','type']).count()['show_id'].reset_index()\n",
        "country_count = {}\n",
        "for i in range(len(country_all)):\n",
        "    c = country_all['country'][i].split(', ')\n",
        "    for x in c:\n",
        "        x = re.sub('[^A-Za-z0-9 ]+', '', x)\n",
        "        if x not in country_count.keys():\n",
        "            country_count[x] = country_all['show_id'][i]\n",
        "        else:\n",
        "          country_count[x] += country_all['show_id'][i]\n",
        "country_df = pd.DataFrame(list(zip(country_count.keys(), country_count.values())), columns =['country', 'count'])\n",
        "#visualization\n",
        "iplot([go.Choropleth(locations = list(country_count.keys()),\n",
        "            locationmode='country names',\n",
        "            z = list(country_count.values()),\n",
        "            text = country_df['country'])])\n",
        ""
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot distribution of content on basis of countries on the world map."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see from the world map that which country produces the how much content.\n",
        "From the chart we can see that United State produces the highets number of content followed by the India"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart have shows the content producing countries on the global map. It will help the company to better understanding of the which countries have more content. and help the insight for company to take business decisions related to production, distribution and marketing."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  9 - Top Directors"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "# Top directors or popular directors on netflix\n",
        "\n",
        "directors = df_copy[df_copy.director != 'unknown'].set_index('title').director.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y=directors, order=directors.value_counts().index[:10], palette='pastel')\n",
        "plt.title('Top 10 Directors')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the Top directors or popular directors present on Netflix"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the top 10 directors are on Netflix with most number of content are mostly International.\n",
        "Jan Suter is top director with high no of content directed on netflix."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This will help the industry to undersrtand the top ten directors that directed the content on Netflix. It will help the insight that the reach out the popular directors for the future content directions that will produce under netflix production,"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  10 - Top Actors"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Top 10 actors in movies based on number movies acted in\n",
        "\n",
        "cast_movies = df_copy[df_copy.cast != 'unknown'].set_index('title').cast.str.split(', ', expand=True).stack().reset_index(level=1, drop=True)\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(y=cast_movies, order=cast_movies.value_counts().index[:10], palette='bright')\n",
        "plt.title('Top 10 Actor acted in Movies on Netflix')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We Visualize the Top 10 actors in movies based on number movies acted in on the Netflix"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Highest number of actors are Indian in the top 10 actors acted in movies present on the Netflix.\n",
        "Top actor who acted highest no of movies is Anupam Kher followed by Shah Rukh Khan."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above chart help the industry to know the which top ten actors are acted in content that present on the Netflix. It will help the industry to understand the popularity of the actors that acted in mostly content on netflix."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### Chart  11 -  Distribution Duration of Movies"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# ploting the duration time for the movies\n",
        "t = df_copy[df_copy['type']=='Movie'].loc[:,['show_id', 'duration']].copy()\n",
        "cast = (t['duration'].to_list())\n",
        "\n",
        "newlist =[]\n",
        "for genre in cast:\n",
        "  newlist.append(int(genre.split(' ')[0]))\n",
        "country_list = pd.DataFrame({'Duration': newlist, 'id': np.arange(0,len(newlist),1)})\n",
        "sns.displot(x='Duration', data=country_list, kind='hist', height=10, aspect=2, bins=50)\n",
        "plt.xlabel('Minutes')\n",
        "plt.xticks(ticks=np.arange(0,country_list.Duration.max()+1,10))\n",
        "plt.title('Distribution of Movie Duration')\n",
        "plt.show();"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize from this chart distribution of Movie Duration(time)."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The highest duration is about *90* to *110* *min* for movies\n",
        "From this distribution of movies length ranges from 3 min to 312 min and from chart we can see that the distribution is almost normally distributed"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above chart shows the average duration of movies distribution it will help to understand the what is most common time duration is available on the platform"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  12 - Top Genre"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Top 10 genre of movies on netflix\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "df_copy[df_copy[\"type\"]==\"Movie\"][\"listed_in\"].value_counts()[:10].plot(kind=\"barh\",color=\"black\")\n",
        "plt.title(\"Top 10 Genres of Movies\",size=18)\n",
        ""
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the barplot to see Top 10 genre of movies present on Netflix."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Top genre that content highets number of content on Netflix is Documentaries.\n",
        "Stand-Up Comedy and Dramas, International Movies genre contains are equal number of content."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart help the Netflix to understand the which genre has more content present on the platform. And it give insight to improve the growth of the content streaming by uploading the popular genre more on platform."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart  13 -  Words Present in Title"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# most occured word in title\n",
        "\n",
        "word_cloud = df_copy['title']\n",
        "text = ''. join(word for word in word_cloud)\n",
        "\n",
        "# stop word list\n",
        "stopword = set(STOPWORDS)\n",
        "\n",
        "# word cloud image\n",
        "\n",
        "word = WordCloud(stopwords= stopword, background_color='Yellow',width=1920,height=1080).generate(text)\n",
        "plt.rcParams['figure.figsize']=(15,7)\n",
        "plt.imshow(word, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the most occured word in title we plot worldcloud."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It look like words like 'Love', 'Last', 'Man', 'de' are most used in titles.\n",
        "There is word 'Christmas' also occured maybe those movies are released on December."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This chart or the wordcloud image helps the Netflix to identify the frequent words that used in titles, Netflix could used its search engine algorithm to return more accurate results. It will help the industry to build the best recommender that provide more personalized experience for the users, that will help the Netflix to increase the subscribers."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "# preparing data for heatmap\n",
        "\n",
        "rating_age = {'TV-PG': 'Older Kids', 'TV-MA': 'Adults', 'TV-Y7-FV': 'Older Kids',\n",
        "          'TV-Y7': 'Older Kids', 'TV-14': 'Teens', 'R': 'Adults', 'TV-Y': 'Kids',\n",
        "          'NR': 'Adults', 'PG-13': 'Teens', 'TV-G': 'Kids', 'PG': 'Older Kids',\n",
        "          'G': 'Kids', 'UR': 'Adults', 'NC-17': 'Adults'}\n",
        "\n",
        "df_copy['rating_age'] = df_copy['rating'].replace(rating_age)\n",
        "# data for correlation\n",
        "\n",
        "df_copy['count'] = 1\n",
        "heatmap = df_copy.groupby('country')[['country', 'count']].sum().sort_values(by='count',ascending=False).reset_index()[:10]\n",
        "heatmap = heatmap[heatmap != 'unknown']\n",
        "heatmap = heatmap['country']\n",
        "\n",
        "corr = df_copy.loc[df_copy['country'].isin(heatmap)]\n",
        "corr = pd.crosstab(corr['country'], corr['rating_age'], normalize = 'index').T\n",
        "corr\n",
        "# Correlation Heatmap visualization code\n",
        "\n",
        "countries =['United States', 'India', 'United Kingdom', 'Canada', 'Japan', 'France', 'South Korea', 'Spain', 'Egypt']\n",
        "rating = ['Adults', 'Teens', 'Older Kids', 'Kids']\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.heatmap(corr.loc[rating, countries], cmap='YlGnBu', linewidth=2.5, fmt='1.0%', annot_kws={'fontsize':12}, annot=True)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We visualize the correlation between the countries and rating of streaming content on Netlfix."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The United state and United Kingdom are close to correlate the rating age means both countries people prefers to watch similar type of content."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.movies for kids and older kids are having at least two hours long.\n",
        "\n",
        "2.movies for kids and older kids are not having two hours long."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypothesis = df_copy\n",
        "hypothesis['rating'].unique()"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis = hypothesis[hypothesis['type'] == 'Movie']\n",
        "\n",
        "rating_age = {'TV-PG': 'Older Kids', 'TV-MA': 'Adults', 'TV-Y7-FV': 'Older Kids',\n",
        "          'TV-Y7': 'Older Kids', 'TV-14': 'Teens', 'R': 'Adults', 'TV-Y': 'Kids',\n",
        "          'NR': 'Adults', 'PG-13': 'Teens', 'TV-G': 'Kids', 'PG': 'Older Kids',\n",
        "          'G': 'Kids', 'UR': 'Adults', 'NC-17': 'Adults'}\n",
        "\n",
        "hypothesis['ages'] = hypothesis['rating'].replace(rating_age)\n",
        "hypothesis['ages'].unique()"
      ],
      "metadata": {
        "id": "zao4OwZPlRdK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypothesis['ages'] = pd.Categorical(hypothesis['ages'], categories=['Kids', 'Older Kids', 'Teens', 'Adults'])\n",
        "hypothesis['duration'] = hypothesis['duration'].astype(str).str.extract('(\\d+)')\n",
        "hypothesis['duration'] = pd.to_numeric(hypothesis['duration'])\n",
        "group_by = hypothesis[['duration', 'ages']].groupby(by='ages')\n",
        "\n",
        "group = group_by.mean().reset_index()\n",
        "group\n",
        ""
      ],
      "metadata": {
        "id": "KTeXVxfQlUqB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "K = group_by.get_group('Kids')\n",
        "O = group_by.get_group('Older Kids')\n",
        "\n",
        "K_M = K.mean()\n",
        "K_S = K.std()\n",
        "\n",
        "O_M = O.mean()\n",
        "O_S = O.std()\n",
        "print('Movies rated for Kids Mean{}\\nMovies rated for older kids Mean{}'.format(K_M,O_M))\n",
        "print('Movies rated for Kids Std{}\\nMovies rated for older kids Std{}'.format(K_S,O_S))"
      ],
      "metadata": {
        "id": "yudO0ojWleh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain t-Value\n",
        "\n",
        "K_n = len(K)\n",
        "O_n = len(O)\n",
        "print(K_n, O_n)\n",
        "\n",
        "dof = K_n + O_n - 2\n",
        "print('dof', dof)\n",
        "\n",
        "sp_2 = ((O_n-1)*K_S**2 + (K_n-1)*O_S**2)/dof\n",
        "print('SP 2 =', sp_2)\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "print('SP', sp)\n",
        "\n",
        "t_value = (K_M - O_M)/(sp*np.sqrt(1/K_n + 1/O_n))\n",
        "print('tvalue', t_value[0])"
      ],
      "metadata": {
        "id": "AmyL4UcBlkoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-distribution\n",
        "stats.t.ppf(0.025,dof)"
      ],
      "metadata": {
        "id": "GNofvdA1lo4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# t-distribution\n",
        "stats.t.ppf(0.975,dof)"
      ],
      "metadata": {
        "id": "Gya2t-rGlr6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-value is not in range so the null hypothesis is  rejected.\n",
        "\n",
        "So, movies rated for kids and older kids are not longer than the two hours."
      ],
      "metadata": {
        "id": "i7hUmVPrlv5u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain t-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the t.ppf function to calculate the value of t for a given probability (q) and degrees of freedom (df)"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a 95% confidence interval, we need to find the cutoff points that enclose 95% of the probability distribution. This can be done using the t.ppf() function with q values of 0.025 and 0.975 for the lower and upper cutoff points respectively. The degrees of freedom (df) argument is usually the sample size minus 1 for a single population sampling problem."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More than 90 mins durations are movies\n",
        "\n",
        "More than 90 mins durations are Not movies"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "hypoth = df_copy\n",
        "\n",
        "hypoth['duration'] = hypoth['duration'].astype(str).str.extract('(\\d+)')\n",
        "hypoth['duration'] = pd.to_numeric(hypoth['duration'])"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hypoth['type'] = pd.Categorical(hypoth['type'], categories=['Movie', 'TV Show'])\n",
        "\n",
        "group_by = hypoth[['duration', 'type']].groupby(by='type')\n",
        "group = group_by.mean().reset_index()\n",
        "group"
      ],
      "metadata": {
        "id": "a-G8mjDt-oPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "M = group_by.get_group('Movie')\n",
        "S = group_by.get_group('TV Show')\n",
        "\n",
        "M_m = M.mean()\n",
        "M_s = M.std()\n",
        "\n",
        "S_m = S.mean()\n",
        "S_s = S.std()\n",
        "\n",
        "print('Mean {}'.format(M_m, S_m))\n",
        "print('Std {}'.format(M_s, S_s))"
      ],
      "metadata": {
        "id": "WYMXibw--3Aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain t-Value\n",
        "\n",
        "M_n = len(M)\n",
        "S_n = len(S)\n",
        "print(M_n, S_n)\n",
        "\n",
        "dof = M_n + S_n - 2\n",
        "print('dof',dof)\n",
        "\n",
        "sp_2 = ((S_n-1)*M_s**2 + (M_n-1)*S_s**2)/dof\n",
        "print('SP_2 =',sp_2)\n",
        "\n",
        "sp = np.sqrt(sp_2)\n",
        "print('SP',sp)\n",
        "\n",
        "t_val = (M_m - S_m)/(sp*np.sqrt(1/M_n + 1/S_n))\n",
        "print('tvalue', t_val[0])"
      ],
      "metadata": {
        "id": "UjJ8zv7v--wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.t.ppf(0.025, dof)"
      ],
      "metadata": {
        "id": "_OgpKBeH_BxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats.t.ppf(0.975, dof)"
      ],
      "metadata": {
        "id": "8kuJEzaR_FC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "t-value are not in range so the *null hypothesis is rejected.\n",
        "\n",
        "So, from that the duration more than the 90 min are movies."
      ],
      "metadata": {
        "id": "Fwj9hayk_I5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the t.ppf function to calculate the value of t for a given probability (q) and degrees of freedom (df)"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get a 95% confidence interval, we need to find the cutoff points that enclose 95% of the probability distribution. This can be done using the t.ppf() function with q values of 0.025 and 0.975 for the lower and upper cutoff points respectively. The degrees of freedom (df) argument is usually the sample size minus 1 for a single population sampling problem."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used fillna and dropna method for the missing values before starting the EDA."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Handling Outliers & Outlier treatments\n",
        "# outlier in release year\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(df_copy.release_year)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replacing ouylier with mean value\n",
        "\n",
        "Q1 = df_copy.release_year.quantile(0.25)\n",
        "Q3 = df_copy.release_year.quantile(0.75)\n",
        "IQR = Q3 - Q1\n",
        "print(f'release_year Q1 ={Q1}\\nrelease_year Q3 = {Q3}\\nrelease_year IQR = {IQR}')"
      ],
      "metadata": {
        "id": "lPIPIXn5_3fc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't have release year greater than 2018"
      ],
      "metadata": {
        "id": "7ccTjDt_AC6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "outlier = df_copy[(df_copy.release_year < (Q1 - 1.5 * IQR)) | (df_copy.release_year > (Q3 + 1.5 * IQR))]\n",
        "\n",
        "df_copy['release_year'] = np.where(df_copy['release_year'] <2009, df_copy.release_year.mean(), df_copy['release_year'])"
      ],
      "metadata": {
        "id": "iymsku7yAHCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.boxplot(df_copy.release_year)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FsMbpKElAKFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is consist of object data type as it most of the column contain textual data / observation. only the release year consit int data type so we check outlier and treated with replacing it with mean value."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.columns"
      ],
      "metadata": {
        "id": "CpNg8uEJAlay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# coverting listed_in into string type\n",
        "df_copy['listed_in'] = df_copy['listed_in'].apply(lambda x: str(x))"
      ],
      "metadata": {
        "id": "iZwRXftVAqpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating new column no_category\n",
        "\n",
        "no_category = []\n",
        "for categories in df_copy.listed_in.values:\n",
        "  len_categories = len(categories.split(','))\n",
        "  no_category.append(len_categories)"
      ],
      "metadata": {
        "id": "JxAXNL2nAvJZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['no_category'] = no_category\n",
        "df_copy[['listed_in', 'no_category']].head()\n",
        ""
      ],
      "metadata": {
        "id": "JiPL70BDAx5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy.no_category.value_counts()"
      ],
      "metadata": {
        "id": "K2gwvSUdA4JB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string"
      ],
      "metadata": {
        "id": "BeutxazHBCQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "\n",
        "\n",
        "def remove_punct(text):\n",
        "  ''' this function is used for removing the\n",
        "  punctuation marks from text'''\n",
        "\n",
        "  for punctuation in string.punctuation:\n",
        "    text = text.replace(punctuation, '')\n",
        "  return text"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['description'] = df_copy['description'].apply(remove_punct)\n",
        "df_copy['description'][0:3]"
      ],
      "metadata": {
        "id": "vE4KXYjzBGsA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def delete_stopwords(description):\n",
        "  ''' this function will take the text data\n",
        "   as input and delete the stopwords from it.'''\n",
        "\n",
        "  clean_text= [i.lower() for i in description.split() if i.lower() not in stop_words]\n",
        "  return ' '.join(clean_text)     # join the list of words with space separator"
      ],
      "metadata": {
        "id": "C80DrkjjBY-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['description'] = df_copy['description'].apply(delete_stopwords)\n",
        "df_copy['description'][0:3]"
      ],
      "metadata": {
        "id": "4O2u77EVBdbx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokenizer = TweetTokenizer()"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# function to lemmatize the corpus\n",
        "\n",
        "def lemmatize_verb(words):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemma = []\n",
        "  for word in words:\n",
        "    lemmas = lemmatizer.lemmatize(word, pos='v')\n",
        "    lemma.append(lemmas)\n",
        "  return lemma"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "\n",
        "vocab = [ ]\n",
        "count_vocab = []\n",
        "\n",
        "for key, value in dictionary:\n",
        "  vocab.append(key)\n",
        "  count_vocab.append(value)\n",
        ""
      ],
      "metadata": {
        "id": "3B4ivA93Cp_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the count in pandas dataframe\n",
        "\n",
        "vocab_stem = pd.DataFrame({'word': vocab, 'count': count_vocab})\n",
        "\n",
        "vocab_stem = vocab_stem.sort_values('count', ascending=False)    # sort the dataframe\n",
        ""
      ],
      "metadata": {
        "id": "wW0jb5zgCson"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_vocab = vocab_stem.head(15)\n",
        "\n",
        "top_words = top_vocab.word.values\n",
        "top_words\n",
        ""
      ],
      "metadata": {
        "id": "ackwxNirCvb4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_count = top_vocab['count'].values\n",
        "top_words_count"
      ],
      "metadata": {
        "id": "rLumUZAwCySq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(top_words, top_words_count)\n",
        "plt.xlim(18135, 18152)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GY8SDBOGC0xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Snowball Stemmer**"
      ],
      "metadata": {
        "id": "esb7hatkDWbK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating stemming function\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def stemming(text):\n",
        "  '''function for each words stem in text'''\n",
        "  text = [stemmer.stem(word) for word in text.split()]\n",
        "  return ' '.join(text)"
      ],
      "metadata": {
        "id": "Y0Bywr5BDgSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming the description\n",
        "\n",
        "df_copy['description'] = df_copy['description'].apply(stemming)\n",
        "df_copy['description'][0:4]\n",
        ""
      ],
      "metadata": {
        "id": "Squ9M8MFDj-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorization**"
      ],
      "metadata": {
        "id": "LIs_nbEsDoJq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TfidfVectorizer\n",
        "\n",
        "# creating the object for Tfid Vectorizer\n",
        "tfid_vect = TfidfVectorizer()\n",
        "# fit the vectorizer\n",
        "tfid_vect.fit(df_copy['description'])\n",
        "# collect the vocabulary item that used in vectorizer\n",
        "dictionary = tfid_vect.vocabulary_.items()\n",
        ""
      ],
      "metadata": {
        "id": "mBF3C_cHDns3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list to store the vocab and count\n",
        "\n",
        "vocab = [ ]\n",
        "count_vocab = []\n",
        "\n",
        "# iterate through each vocab and count and append to the lists.\n",
        "for key, value in dictionary:\n",
        "  vocab.append(key)\n",
        "  count_vocab.append(value)"
      ],
      "metadata": {
        "id": "j98sU9C1DvBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# store the count in panda dataframe\n",
        "\n",
        "vocab_after_stem = pd.DataFrame({'word': vocab, 'count': count_vocab})\n",
        "\n",
        "vocab_after_stem = vocab_after_stem.sort_values('count', ascending=False)\n",
        ""
      ],
      "metadata": {
        "id": "axuFl5nzDx5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_voacb_after = vocab_after_stem.head(15)\n",
        "\n",
        "top_words_after = top_voacb_after.word.values\n",
        "top_words_after"
      ],
      "metadata": {
        "id": "zXSQL8b6D0o3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_words_count_after = top_voacb_after['count'].values\n",
        "top_words_count_after"
      ],
      "metadata": {
        "id": "j7twsQ8ZD3hJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.barh(top_words_after, top_words_count_after)\n",
        "plt.xlim(14237, 14253)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "sYNMpVdCD5_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding new column length of description\n",
        "\n",
        "df_copy['length(desc)'] = df_copy['description'].apply(lambda x: len(x))"
      ],
      "metadata": {
        "id": "GFREeJ2QEKMC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Listed_in column"
      ],
      "metadata": {
        "id": "4IljNdq4ENTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing punctuations from listed_in column\n",
        "\n",
        "df_copy['listed_in'] = df_copy['listed_in'].apply(remove_punct)"
      ],
      "metadata": {
        "id": "9sOhdzuvEOGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords from listed_in column\n",
        "\n",
        "df_copy['listed_in'] = df_copy['listed_in'].apply(delete_stopwords)"
      ],
      "metadata": {
        "id": "coD82cthETBY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the countvectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(df['listed_in'])\n",
        "dict_listed = count_vect.vocabulary_.items()\n",
        "dict_listed\n",
        ""
      ],
      "metadata": {
        "id": "uBlTNbreEWC5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_vocab = []\n",
        "\n",
        "for key, value in dict_listed:\n",
        "  vocab.append(key)\n",
        "  count_vocab.append(value)\n",
        ""
      ],
      "metadata": {
        "id": "HRb49uXUEY9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_listed_stem = pd.DataFrame({'word': vocab, 'count': count_vocab})\n",
        "\n",
        "vocab_listed_stem = vocab_listed_stem.sort_values('count', ascending=False)\n",
        ""
      ],
      "metadata": {
        "id": "y8E-1mJXEbZn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_listed_vocab = vocab_listed_stem.head(15)\n",
        "\n",
        "top_listed_words = top_listed_vocab.word.values\n",
        "top_listed_words"
      ],
      "metadata": {
        "id": "vV-LUTcdEdtP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_listed_words_count = top_listed_vocab['count'].values\n",
        "top_listed_words_count"
      ],
      "metadata": {
        "id": "CyVsIynzEgCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(top_listed_words, top_listed_words_count)\n",
        "plt.xlim(28, 45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iM_lNjBPEinX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**stemming**"
      ],
      "metadata": {
        "id": "PkD6Z3XXEt6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stemming\n",
        "\n",
        "df_copy['listed_in'] = df_copy['listed_in'].apply(stemming)\n",
        "df_copy['listed_in'][0:2]"
      ],
      "metadata": {
        "id": "25gDLwIVEwAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Vectorization**"
      ],
      "metadata": {
        "id": "pECMOJFYE0US"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the object for Tfidf Vectorizer\n",
        "\n",
        "tfid_vect = TfidfVectorizer()\n",
        "tfid_vect.fit(df_copy['listed_in'])\n",
        "dict_listed = tfid_vect.vocabulary_.items()\n",
        "dict_listed"
      ],
      "metadata": {
        "id": "ZDVk4NVhE2Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = [ ]\n",
        "count_vocab = []\n",
        "\n",
        "for key, value in dict_listed:\n",
        "  vocab.append(key)\n",
        "  count_vocab.append(value)\n",
        ""
      ],
      "metadata": {
        "id": "rBk-h-LBE6M3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_listed_after_stem = pd.DataFrame({'word': vocab, 'count': count_vocab})\n",
        "\n",
        "vocab_listed_after_stem = vocab_listed_after_stem.sort_values('count', ascending=False)"
      ],
      "metadata": {
        "id": "h_Yisa8AE8mQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_listed_after_vocab = vocab_listed_after_stem.head(15)\n",
        "top_listed_after_words = top_listed_after_vocab.word.values\n",
        "top_listed_after_words"
      ],
      "metadata": {
        "id": "Wx_hbN_-E_H3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_listed_after_words_count = top_listed_after_vocab['count'].values\n",
        "top_listed_after_words_count"
      ],
      "metadata": {
        "id": "b9ms-sIZFBwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,5))\n",
        "plt.barh(top_listed_after_words, top_listed_after_words_count)\n",
        "plt.xlim(25, 40)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "K0EZPELCFEzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adding new column length of listed_in\n",
        "\n",
        "df_copy['length(list)'] = df_copy['listed_in'].apply(lambda x:len(x))"
      ],
      "metadata": {
        "id": "jWA0JHcDFIVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing Stopwords\n",
        "\n",
        "Removing Punctuations\n",
        "\n",
        "Lowering the case\n",
        "\n",
        "Text Vectorization (Count & TF-IDF)\n",
        "\n",
        "Stemming (Snowball Stemmer)\n",
        "\n",
        "This are the techniques we have used for the text normalization, we have used snowball stemmer for stemming. It is used for reducing a word to its base word or stem in such a way that the words of similar kind lie under a common stem."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "# Vectorizing Text\n",
        "# creating the countvectorizer\n",
        "\n",
        "count_vect = CountVectorizer()\n",
        "count_vect.fit(df['description'])\n",
        "dictionary = count_vect.vocabulary_.items()\n",
        "dictionary"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the count vectorization technique for the text vectorization. It is one of the simplest way of doing text vectorization.\n",
        "\n",
        "Count vectorization fit and learn the word vocabulary and try to create a document term matrix in which the individual cells denote the frequency of that word in a particular document\n",
        "\n",
        "We will also using the TF-IDF vectorization which is term frequency-inverse document frequency, it gives a measure that takes the importance of a word into consideration depending on how frequently it occurs in a document.\n",
        "\n",
        "TF-IDF is similar to the count vectorization method, in the TF-IDF method, a document term matrix is generated and each column represents an individual unique word."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# crating the feature variable\n",
        "\n",
        "feature = df_copy[['no_category', 'length(desc)', 'length(list)']]\n",
        ""
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "scaler = preprocessing.StandardScaler()\n",
        "\n",
        "x_scale = scaler.fit_transform(feature)\n",
        "X = x_scale\n",
        "silhouette_score_ = []\n",
        "range_cluster = [i for i in range(2,18)]"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have used the StandardScaler() method for the scaling the data"
      ],
      "metadata": {
        "id": "7hiF7CC5FeZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model  1 - **Silhouette Score**"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "for clusters in range_cluster:\n",
        "  Kmean_cluster = KMeans(n_clusters = clusters)\n",
        "  pred = Kmean_cluster.fit_predict(X)\n",
        "  center = Kmean_cluster.cluster_centers_\n",
        "\n",
        "  score = silhouette_score(X, pred)\n",
        "  silhouette_score_.append([int(clusters), round(score, 3)])\n",
        "  print('For n_clusters = {}, silhouette score {}'. format(clusters, score))\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clust = pd.DataFrame(silhouette_score_, columns=['n clusters', 'silhouette score'])\n",
        "clust = clust.sort_values('silhouette score', ascending=False)\n",
        "clust.head()"
      ],
      "metadata": {
        "id": "AUrtPDNeF1nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "range_cluster = [i for i in range(2,17)]\n",
        "\n",
        "for clusters in range_cluster:\n",
        "  fig, (ax1, ax2) = plt.subplots(1,2)  # creating the subplot\n",
        "  fig.set_size_inches(12,6)\n",
        "\n",
        "  ax1.set_xlim([-0.1, 1])  # 1st subplot is silhouette plot\n",
        "  ax1.set_ylim([0, len(X)+(clusters+1)*10])\n",
        "\n",
        "  Kmean_cluster = KMeans(n_clusters=clusters, random_state=10)\n",
        "  clust_labels = Kmean_cluster.fit_predict(X)\n",
        "\n",
        "  # average values for silhouette score\n",
        "  silhouette_avg = silhouette_score(X, clust_labels)\n",
        "  print('For n_clusters =', clusters, 'The avg silhouette_score is :', silhouette_avg)\n",
        "\n",
        "  # silhiuette score for each sample\n",
        "  sample_silhouette_values = silhouette_samples(X, clust_labels)\n",
        "\n",
        "  y_lower = 10\n",
        "  for i in range(clusters):\n",
        "    ith_cluster_silhouette_values = sample_silhouette_values[clust_labels == i]\n",
        "\n",
        "    ith_cluster_silhouette_values.sort()\n",
        "\n",
        "    size_clust_i = ith_cluster_silhouette_values.shape[0]\n",
        "    y_upper = y_lower + size_clust_i\n",
        "\n",
        "    color = cm.nipy_spectral(float(i)/clusters)\n",
        "    ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "\n",
        "    ax1.text(-0.05, y_lower + 0.5 * size_clust_i, str(i))\n",
        "    y_lower = y_upper + 10\n",
        "\n",
        "  ax1.set_title('Silhouette plot for Various clusters')\n",
        "  ax1.set_xlabel('Slihouette coefficient values')\n",
        "  ax1.set_ylabel('Cluster label')\n",
        "\n",
        "  ax1.axvline(x=silhouette_avg, color='red', linestyle='--')\n",
        "  ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "  ax1.set_yticks([])\n",
        "\n",
        "  # 2nd subplot showing the actual cluster\n",
        "  color = cm.nipy_spectral(clust_labels.astype(float)/clusters)\n",
        "  ax2.scatter(X[:, 0], X[:, 1], marker='.', s=30, lw=0, alpha=0.7, c=color, edgecolor='k')\n",
        "\n",
        "  center = Kmean_cluster.cluster_centers_\n",
        "  ax2.scatter(center[:, 0], center[:, 1], marker='o', c='white', alpha=1, s=200, edgecolor='k')\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "_v9LnaUIHCGX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "kmean = KMeans(n_clusters=17, init = 'k-means++', random_state=50)\n",
        "y_pred = kmean.fit_predict(X)\n",
        "\n",
        "# silhouette score\n",
        "score = silhouette_score(X, y_pred)\n",
        "print('Silhouette score is {}'.format(score))\n",
        ""
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import davies_bouldin_score\n",
        "davies_bouldin_score(X, y_pred)\n",
        ""
      ],
      "metadata": {
        "id": "IeVJN2MZRuwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['cluster'] = y_pred\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='cluster', hue ='type', lw=5, data=df_copy)"
      ],
      "metadata": {
        "id": "gpG5QVh5RxxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster one has highest number of data point for movies and for TV Shows cluster number 13 content highest clusters.\n",
        "\n",
        "Slihouette score is used to evaluate the quality of clusters that crated by using clustering algorithms such as K-means. It shows the how well samples are clustered with other samples that are similar to each other.\n",
        "\n",
        "Silhouette Score can be used to study the separation distance between the resulting clusters. The plot of silhouette shows th a measure of how close each point in one cluster.\n",
        "\n",
        "The value of silhouette coefficient is [-1, 1]. A score 1 denotes the data point is very compact within cluster which is good and it far away from the other clusters. The worst value is -1. Values near 0 denotes overlapping cluster."
      ],
      "metadata": {
        "id": "hecBupLbSN0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model  2 - **Elbow Method**"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 2 Implementation\n",
        "\n",
        "# elbow method\n",
        "sum_sq_dist = {}\n",
        "for k in range(1,15):\n",
        "  Kmean = KMeans(n_clusters=k, init='k-means++', max_iter=1000)\n",
        "  Kmean = Kmean.fit(X)\n",
        "  sum_sq_dist[k] = Kmean.inertia_"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting the graph for the sum square distance value and no of clusters\n",
        "sns.pointplot(x=list(sum_sq_dist.keys()), y=list(sum_sq_dist.values()))\n",
        "plt.xlabel('Number of Cluster')\n",
        "plt.ylabel('Sum of Square Distance')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LBJf_z0ySv0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the graph we will take number of cluster as 3"
      ],
      "metadata": {
        "id": "fdHRVk4gSznE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# we will use 3 cluster\n",
        "\n",
        "Kmean_clust = KMeans(n_clusters = 3)\n",
        "Kmean_clust.fit(X)\n",
        "Kmeans = Kmean_clust.predict(X)\n",
        ""
      ],
      "metadata": {
        "id": "Uzam7JekS2oC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.title('description and listed_in')\n",
        "plt.scatter(X[:, 0], X[:, 1], c=Kmeans, s=50, cmap='spring')\n",
        "\n",
        "centers = Kmean_clust.cluster_centers_\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
        "plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "W-x4kyRmS5OK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Means elbow visualizer shows the elbow method of selecting the optimal number of cluster for K-Means clustering.\n",
        "\n",
        "When all the metrics are plotted. It gives best value for k, the line chart look like an arm and then elbow(the point of inflection on the curve) is the best value of k. The arm can beeither up or down, but if there is strong inflection point then it is good indication that the underlying model fits best at the point."
      ],
      "metadata": {
        "id": "eUSMFSfgS-nN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contentwise Cluster**"
      ],
      "metadata": {
        "id": "0h_VWxTaTEEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contentwise clustering for Kmeans\n",
        "\n",
        "df_copy['clusters'] = Kmean.labels_\n",
        "\n",
        "def word_count(category):\n",
        "  print('Exploring Cluster', category)\n",
        "  column = ['type', 'title', 'country', 'rating', 'listed_in', 'description']\n",
        "  for i in column:\n",
        "    word_cloud = df_copy[['clusters',i]].dropna()\n",
        "    word_cloud = word_cloud[word_cloud['clusters']==category]\n",
        "    text = ''.join(word for word in word_cloud[i])\n",
        "\n",
        "    stopwords = set(STOPWORDS)\n",
        "\n",
        "    wordcloud = WordCloud(stopwords = stopwords, background_color='black',width=1920,height=1080).generate(text)\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = (10,6)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "\n",
        "    print('\\u001b[1m ''Looking for insight from', i, 'of Movies/TV Show')\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "Y8n9TrrKTINU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worldcloud for cluster 2\n",
        "\n",
        "word_count(2)"
      ],
      "metadata": {
        "id": "Yw2Tzt2TTNYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worldcloud for cluster 3\n",
        "\n",
        "word_count(3)"
      ],
      "metadata": {
        "id": "BlcACAYkTZ5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have creat the function for building a wordcloud for cloumns as 'type', 'title', 'country', 'rating', 'listed_in', 'description'. using kmeans clustering algorithm"
      ],
      "metadata": {
        "id": "pGxOiRrBTgRK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model  3 -  **Hierarchical Clustering**"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "# ML Model - 3 Implementation\n",
        "\n",
        "import scipy.cluster.hierarchy as sch\n",
        "\n",
        "hierar = sch.linkage(X, method='ward', metric='euclidean')\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dendogram\n",
        "\n",
        "plt.figure(figsize=(20,12))\n",
        "dendo = sch.dendrogram(hierar)\n",
        "plt.xlabel('Content')\n",
        "plt.ylabel('Euclidean Distance')\n",
        "plt.title('Dendrogram')\n",
        "plt.axhline(y =20, color='r', linestyle='--')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CNEf79vBTqYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dendrogram is tree diagram used to display the relationship between the data points in hierarchical clustering algorithm."
      ],
      "metadata": {
        "id": "a6bS6QJbT0uz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model  3 -  **Agglomerative Clustering**\n",
        "\n"
      ],
      "metadata": {
        "id": "vC5WNppIVkN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agglomerative clustering\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "hierarchical = AgglomerativeClustering(n_clusters=14, affinity='euclidean', linkage='ward')\n",
        "y_pre = hierarchical.fit_predict(X)"
      ],
      "metadata": {
        "id": "T-XKtFoIVqnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# silhouette score\n",
        "score = silhouette_score(X, y_pre)\n",
        "print('Silhouette score is {}'.format(score))"
      ],
      "metadata": {
        "id": "vMizY8-SVuJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "davies_bouldin_score(X, y_pre)"
      ],
      "metadata": {
        "id": "mNpEGCjfVwva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_copy['hierarchical_cluster'] = hierarchical.labels_\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "sns.countplot(x='hierarchical_cluster',hue ='type', lw=5, data=df_copy)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G6smANlmVzM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cluster one has highest number of data point for movies and for TV Shows cluster number eight content highest clusters.\n",
        "\n",
        "We have bulid the cluster using Agglomerative clustering algorithm.\n",
        "\n",
        "We build the 14 clusters successfully using agglomerative clustering."
      ],
      "metadata": {
        "id": "hK1fduPFV2j0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Contentwise Cluster**"
      ],
      "metadata": {
        "id": "B05eeU6oV9Ry"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# contentwise clustering for hierarchical\n",
        "\n",
        "df_copy['hierarchical_cluster'] = hierarchical.labels_\n",
        "\n",
        "def word_count(category):\n",
        "  print('Exploring Cluster', category)\n",
        "  column = ['type', 'title', 'country', 'rating', 'listed_in', 'description']\n",
        "  for i in column:\n",
        "    word_cloud = df_copy[['hierarchical_cluster',i]].dropna()\n",
        "    word_cloud = word_cloud[word_cloud['hierarchical_cluster']==category]\n",
        "    text = ''.join(word for word in word_cloud[i])\n",
        "\n",
        "    stopwords = set(STOPWORDS)\n",
        "\n",
        "    wordcloud = WordCloud(stopwords = stopwords, background_color='black',width=1920,height=1080).generate(text)\n",
        "\n",
        "    plt.rcParams['figure.figsize'] = (10,6)\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "\n",
        "    print('\\u001b[1m ''Looking for insight from', i, 'of Movies/TV Show')\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "oua0oosqWBar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worldcloud for cluster 2\n",
        "\n",
        "word_count(2)\n",
        ""
      ],
      "metadata": {
        "id": "7TYufPL_WGqy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# worldcloud for cluster 4\n",
        "\n",
        "word_count(4)"
      ],
      "metadata": {
        "id": "POhpAmc8WLmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have creat the function for building a wordcloud for cloumns as 'type', 'title', 'country', 'rating', 'listed_in', 'description'. using hierarchical clustering algorithm"
      ],
      "metadata": {
        "id": "t13AO6kpWZWB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Content Based Recommendation System**"
      ],
      "metadata": {
        "id": "QIzzDFaBWeip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing libraries\n",
        "\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "import re, string, unicodedata\n",
        "import nltk\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "string.punctuation\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "NtSmLEjLWneG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loading the dataset\n",
        "recom_df = df.copy()\n",
        "recom_df.head(5)\n",
        ""
      ],
      "metadata": {
        "id": "_U8U_JfXWuOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handling the Null/Missing Values\n",
        "\n",
        "recom_df[['cast', 'director', 'country']]=recom_df[['cast', 'director', 'country']].fillna('unknown')\n",
        "recom_df['rating'].fillna(recom_df['rating'].mode()[0], inplace=True)\n",
        "recom_df.dropna(axis=0, inplace=True)\n",
        "recom_df.fillna('', inplace=True)\n",
        ""
      ],
      "metadata": {
        "id": "a8lOWmZuWxvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# wrangling the data\n",
        "\n",
        "recom_df['country'] = recom_df['country'].apply(lambda x: x.split(',')[0])\n",
        "recom_df['listed_in'] = recom_df['listed_in'].apply(lambda x: x.split(',')[0])\n",
        "recom_df['duration'] = recom_df['duration'].apply(lambda x:int(x.split()[0]))"
      ],
      "metadata": {
        "id": "PqBRPsgxW0Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**creating feature**"
      ],
      "metadata": {
        "id": "WlAyWGFcW3Y6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# combine the clustering variable in one column\n",
        "\n",
        "recom_df['clustering_var'] = (recom_df['director']+' '+ recom_df['cast']+' '+\n",
        "                             recom_df['country']+' '+ recom_df['listed_in']+' '+\n",
        "                             recom_df['description'])"
      ],
      "metadata": {
        "id": "2tWnyTZkW8QU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Textual Data Preprocessing**"
      ],
      "metadata": {
        "id": "caXEPEhhW_gZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing the non-ASCII characters\n",
        "\n",
        "def remove_non_ascii(words):\n",
        "  new_words = []\n",
        "  for word in words:\n",
        "    new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    new_words.append(new_word)\n",
        "  return new_words\n",
        "\n",
        "recom_df['clustering_var'] = remove_non_ascii(recom_df['clustering_var'])"
      ],
      "metadata": {
        "id": "nRjRqvNMXx2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords and lower case\n",
        "\n",
        "\n",
        "\n",
        "def stopwords(text):\n",
        "  text = [word.lower() for word in text.split() if word.lower() not in sw]\n",
        "  return ' '.join(text)\n",
        "\n",
        "recom_df['clustering_var'] = recom_df['clustering_var'].apply(stopwords)\n",
        ""
      ],
      "metadata": {
        "id": "E64wrF-DYAIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing punctuations\n",
        "\n",
        "def remove_pun(text):\n",
        "  trans = str.maketrans('', '', string.punctuation)\n",
        "  return text.translate(trans)\n",
        "\n",
        "recom_df['clustering_var'] = recom_df['clustering_var'].apply(remove_pun)\n",
        ""
      ],
      "metadata": {
        "id": "qH97vFWiYdoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# function to lemmatize the corpus\n",
        "\n",
        "def lemmatize_verb(words):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemma = []\n",
        "  for word in words:\n",
        "    lemmas = lemmatizer.lemmatize(word, pos='v')\n",
        "    lemma.append(lemmas)\n",
        "  return lemma\n",
        "\n",
        "recom_df['clustering_var'] = lemmatize_verb(recom_df['clustering_var'])\n",
        ""
      ],
      "metadata": {
        "id": "MPtPSpAcYg9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenization\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "recom_df['clustering_var'] = recom_df['clustering_var'].apply(lambda x: tokenizer.tokenize(x))"
      ],
      "metadata": {
        "id": "jkHr_2ICYk4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparing data for function"
      ],
      "metadata": {
        "id": "QA0BQIreYodC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# changing index from show_id to show title\n",
        "recom_df['show_id'] = recom_df.index\n",
        "\n",
        "# convert token to string\n",
        "def convert(lis):\n",
        "  return ' '.join(lis)\n",
        "\n",
        "recom_df['clustering_var'] = recom_df['clustering_var'].apply(lambda x: convert(x))"
      ],
      "metadata": {
        "id": "hzw9q8qIYqjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# changing the index to title of movies/tv shows\n",
        "recom_df.set_index('title', inplace =True)"
      ],
      "metadata": {
        "id": "kgsdj8CzYu20"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# count vectorizer\n",
        "CV = CountVectorizer()\n",
        "convert_matrix = CV.fit_transform(recom_df['clustering_var'])"
      ],
      "metadata": {
        "id": "Z-wlGaaFY18T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cosine similarity\n",
        "cosine_similarity = cosine_similarity(convert_matrix)"
      ],
      "metadata": {
        "id": "3-bfx6KUZAqJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for Recommendation system**"
      ],
      "metadata": {
        "id": "3C_xTTVYZL1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating the function to get 10 recommendation for show\n",
        "\n",
        "indices = pd.Series(recom_df.index)\n",
        "\n",
        "def recomm_10(title, cos_sim = cosine_similarity):\n",
        "  try:\n",
        "    recomm_content = []\n",
        "    id = indices[indices == title].index[0]\n",
        "    series = pd.Series(cos_sim[id]).sort_values(ascending=False)\n",
        "    top_10 = list(series.iloc[1:11].index)\n",
        "\n",
        "    for i in top_10:\n",
        "      recomm_content.append(list(recom_df.index)[i])\n",
        "    print('If you liked {}'.format(title), 'you may also like:\\n')\n",
        "    return recomm_content\n",
        "\n",
        "  except:\n",
        "    return 'Invalid Entry'"
      ],
      "metadata": {
        "id": "oW1kiofcZOav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**"
      ],
      "metadata": {
        "id": "BPPapqN_ZSFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking recommendation system\n",
        "\n",
        "recomm_10('Stranger Things')"
      ],
      "metadata": {
        "id": "9vv63LsRZUVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recomm_10('Zubaan')"
      ],
      "metadata": {
        "id": "muaiMFa6ZaSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recomm_10('Game of Thrones')"
      ],
      "metadata": {
        "id": "3OBWFLOxZd3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have build a simple content based recommendation system based on the similarity of the show.\n",
        "\n",
        "To get the similarity score of the contents, we have used cosine similarity\n",
        "\n",
        "The similarity of two vectors (A and B) is calculated by taking the dot product of the two vectord and dividing it by the magnitude values."
      ],
      "metadata": {
        "id": "W2edWtQ7ZheZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the clustering analysis we can use topic modeling.\n",
        "\n",
        "Can be apply different clustering algorithms. More time could be given on clustering analysis.\n",
        "\n",
        "Can be improve the recommender system and building a working app on this recommender system."
      ],
      "metadata": {
        "id": "koB6nG5NZrnZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we have done work on the text clustering problem where we had group the Netflix shows into clusters such that the content within a cluster are similar to each other and the content that are differnt clusters are dissimilar to each other. We have concluded as follow:"
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploratory Data Analysis (EDA)**"
      ],
      "metadata": {
        "id": "uM6DfvWWZ0L4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The most content type is from Movies on Netflix. 39% of content is from \"TV Show\" type and 61% is from \"Movies\".\n",
        "\n",
        "The highest count content on Netflix is made with \"TV-MA\" rating.\n",
        "\n",
        "The country that produced highest number of content is United State followed by India.\n",
        "\n",
        "The most popular director with highest number of content directed is Jan Suter\n",
        "\n",
        "The most popular actor with highest number of content acted in is Anupam Kher.\n",
        "\n",
        "Most of the content from the genre is \"International Movies\""
      ],
      "metadata": {
        "id": "Zjvd4NTHZ7Zx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clustering**"
      ],
      "metadata": {
        "id": "alMBUUpcaGrY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have cluster the data before that we have preprocessed the data and then vectorized using countvectorizer and TFIDF vectorizer.\n",
        "\n",
        "We have built cluster using K-means clustering algorithm we obtained clusters from Slihouette score and elbow method analysis.\n",
        "\n",
        "Then clusters where built on Hierarchical clustering algorithm we have obtained optimal number of clusters after visualizing the dendrogram."
      ],
      "metadata": {
        "id": "3XB_z5U8aH53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recommendation **System**"
      ],
      "metadata": {
        "id": "Gg7jjJjBaO-Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we build content based recommender system using cosine similarity. This recommender system will make 10 recommendation to the user based on the show they watched."
      ],
      "metadata": {
        "id": "0zkDCDTQaQUz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}